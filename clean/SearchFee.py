import CreateData as CD
import pyspark.sql.functions as F
from pyspark.sql import SparkSession
from pyspark.sql.types import DecimalType, IntegerType

"""
è¦ç”¨åˆ°çš„éƒ¨åˆ†è¡¨å¤´ï¼š
    TotalFeeï¼šæ€»è´¹ç”¨
    RealCompï¼šå®é™…è¡¥å¿è´¹ç”¨
    SelfPayï¼šæ‚£è€…è‡ªä»˜è´¹ç”¨
    Countï¼šæŸç§è¯çš„å¼€è¯æ•°ç›®
    FeeSumï¼šè´¹ç”¨é‡‘é¢
    AllowedCompï¼šå¯è¡¥å¿é‡‘é¢
    UnallowedCompï¼šä¸å¯è¡¥å¿é‡‘é¢
    CompRatioï¼šæŠ¥é”€æ¯”ä¾‹
    DTï¼šæ•°æ®å¹´ä»½
"""

# åŠŸèƒ½
'''
# TODO:ç»Ÿè®¡è¿‘ä¸‰å¹´çš„åŒ»ä¿è´¹ç”¨å¼€æ”¯(ä»¥å¹´ä¸ºå•ä½æ˜¾ç¤º)--->RealComp, DT
# TODO:ç»Ÿè®¡è¿‘ä¸‰å¹´è¯å“å¼€æ”¯æƒ…å†µ(ä»¥å¹´ä¸ºå•ä½æ˜¾ç¤º)--->FeeSum, Count, DT 
# TODO:ç»Ÿè®¡è¿‘ä¸‰å¹´åŒ»ä¿è¯å“å¼€æ”¯æƒ…å†µ(å³æ”¯ä»˜é‡‘é¢,æ•°é‡;ä»¥å¹´ä¸ºå•ä½æ˜¾ç¤º)--->AllowedComp, CopRatio, Count, DT
# TODO:ç»Ÿè®¡è¿‘ä¸‰å¹´æ‰¶è´«äººå£æ‚£ç—…çš„åœ°ç†åˆ†å¸ƒ,å¹´é¾„åˆ†å¸ƒ
'''


# TODO:ç»Ÿè®¡è¿‘ä¸‰å¹´æŸäº›åœ°åŒºçš„æ‰¶è´«äººå£çš„æ‚£ç—…å¹´é¾„åˆ†å¸ƒä»¥åŠç”¨è¯ç±»å‹
# æœºå™¨å­¦ä¹ 
# TODO:é¢„æµ‹æœªæ¥å‡ å¹´åŒ»ä¿æ‰¶è´«è´¹ç”¨çš„æŒç»­å¼€æ”¯æƒ…å†µ
# TODO:é¢„æµ‹æœªæ¥å‡ å¹´æ‰¶è´«äººå£æ‚£ç—…äººç¾¤ä¸»è¦ç”¨è¯æƒ…å†µ(çº³å…¥åŒ»ä¿è¡¥å¿èŒƒå›´)
# TODO:é¢„æµ‹æœªæ¥å‡ å¹´æ‰¶è´«äººå£æ‚£ç—…äººç¾¤çš„åˆ†å¸ƒä½ç½®


# æŸ¥è¯¢ç»Ÿè®¡è¿‘å‡ å¹´çš„åŒ»ä¿å¼€é”€
def getDataMed(path):
    data = spark.read.format('parquet').load(path).select('HosRegisterCode', 'RealComp', 'DT')
    medFee = data.dropDuplicates(subset=['HosRegisterCode']).drop(
        'HosRegisterCode')
    # è®¡ç®—è´¹ç”¨
    allFee = medFee.groupby('DT').agg(F.sum('RealComp').alias('Fee')).orderBy(medFee.DT.desc())
    # é¿å…ç§‘å­¦è®¡æ•°æ³•æ˜¾ç¤º
    allFee = allFee.withColumn('Fee', allFee.Fee.cast(DecimalType(18, 2)))
    allFee.show()


# è·å¾—è¿‘ä¸‰å¹´çš„å»ºæ¡£ç«‹å¡äººå£çš„è¯å“è¡¥åŠ©å¼€æ”¯æƒ…å†µ
def getDataDrugAllowed(path):
    # è·å¾—äººå‘˜ç±»å‹ï¼ˆå»ºæ¡£ç«‹å¡ï¼‰,æ ¹æ®äººå‘˜ç±»å‹ç»Ÿè®¡ç”¨è¯è´¹ç”¨
    data = spark.read.format('parquet').load(path).select('PersonalType', 'AllowedComp', 'DT')
    data = data.where(data.PersonalType == '17') \
        .groupby('DT') \
        .agg(F.sum(data.AllowedComp)
             .alias('Fee')) \
        .orderBy(
        data.DT.desc())
    fee = data.withColumn('Fee', data.Fee.cast(DecimalType(18, 2)))
    fee.show()


# è·å–ğŸ¥‡ä¸‰å¹´æ¥å»ºæ¡£ç«‹å¡äººå£çš„è¯å“ä½¿ç”¨æƒ…å†µ
def getDataDrugPoor(path):
    data = spark.read.format('parquet').load(path).select('PersonalType', 'DrugName', 'DT', 'Count')
    data = data.where((data.PersonalType == '17') & (data.DrugName != '0')) \
        .withColumn('DrugName', CD.changeNameUDF(data.DrugName)) \
        .drop('PersonalType') \
        .withColumn('Count', data.Count.cast(IntegerType()))
    dataDrug = data.groupby('DrugName') \
        .pivot('DT', ['2017', '2018', '2019']) \
        .agg(F.sum('Count')) \
        .fillna(0)
    dataDrug = dataDrug.orderBy(dataDrug['2019'].desc())
    dataDrug.show(50)


# ç»Ÿè®¡è¿‘ä¸‰å¹´æ‚£ç—…æ‰¶è´«äººå£çš„åœ°ç†åˆ†å¸ƒï¼šæ€»å…±21335äººæ¬¡
def getDataPlacePoor(path):
    path01 = 'hdfs://localhost:9000/csv/Join_Canton'
    data01 = spark.read.format('csv').option('header', 'true').load(path01) \
        .where('Level= 04').drop('UpperCode', 'AllName', 'DT') \
        .withColumn('CantonCode', F.split('CantonCode', '\d{4}$')) \
        .withColumn('CantonCode', F.concat_ws("", "CantonCode"))

    data01 = data01.withColumn("CantonName", F.when(data01.CantonName == 'å»ºæ¡£ç«‹å¡äººå‘˜', 'å›ºé˜³å¿å»ºæ¡£ç«‹å¡äººå‘˜') \
                               .otherwise(data01.CantonName))

    data = spark.read.format('parquet').load(path) \
        .select('PersonalType', 'AllName', 'DT', 'HosRegisterCode', 'CantonCode') \
        .dropDuplicates(subset=['HosRegisterCode']) \
        .where('PersonalType = 17') \
        .withColumn('CantonCode', F.split('CantonCode', '\d{4}$')) \
        .withColumn('CantonCode', F.concat_ws("", "CantonCode"))

    data = data.join(data01, on='CantonCode', how='left_outer') \
        .drop('PersonalType', 'Level', 'CantonCode', 'AllName', 'ZoneCode', 'HosRegisterCode') \
        .dropDuplicates(subset=['HosRegisterCode']) \
        .withColumn('Times', F.lit(1)) \
        .groupby('CantonName') \
        .pivot('DT', ['2017', '2018', '2019']) \
        .agg(F.sum('Times')) \
        .fillna(0)
    data = data.orderBy(data['2019'].desc())
    data.show(50)
    data.groupby().sum().show()  # å’Œä¸º21335


# ç»Ÿè®¡è¿‘ä¸‰å¹´æ‚£ç—…æ‰¶è´«äººå£çš„å¹´é¾„åˆ†å¸ƒ
def getDataAgePoor(path):
    data = spark.read.format('parquet').load(path) \
        .select('PersonalType', 'DT', 'HosRegisterCode', 'Age') \
        .dropDuplicates(subset=['HosRegisterCode']) \
        .where('PersonalType = 17') \
        .drop("HosRegisterCode", "PersonalType") \
        .withColumn('Count', F.lit(1))
    data = data.groupBy('Age') \
        .pivot('DT', ['2017', '2018', '2019']) \
        .agg(F.sum('Count')).fillna('0') \
        .fillna(0)
    data = data.withColumn('Age', data.Age.cast(IntegerType())) \
        .orderBy(data['2019'].desc())
    data.show()
    data.groupby().sum().show()


# TODO:ç»Ÿè®¡è¿‘ä¸‰å¹´æŸäº›åœ°åŒºçš„æ‰¶è´«äººå£çš„æ‚£ç—…å¹´é¾„åˆ†å¸ƒä»¥åŠç”¨è¯ç±»å‹
def getDataAgeInPlace(path):
    data = spark.read.format('parquet').load(path)


if __name__ == '__main__':
    # spark = SparkSession.builder \
    #     .master("local") \
    #     .appName("example") \
    #     .config("spark.debug.maxToStringFields", "100") \
    #     .config("spark.sql.shuffle.partitions", "400") \
    #     .config("spark.default.parallelism", "600") \
    #     .config("spark.sql.auto.repartition", "true") \
    #     .config("spark.sql.execution.arrow.enabled", "true") \
    #     .enableHiveSupport() \
    #     .getOrCreate()

    # æ€»è¡¨è·¯å¾„
    spark = SparkSession.builder \
        .master("local") \
        .appName("example") \
        .config("spark.debug.maxToStringFields", "1000") \
        .getOrCreate()

    inputFile = 'hdfs://localhost:9000/result/form_par'
    # getDataMed(inputFile)
    # getDataDrugAllowed(inputFile)
    # getDataDrugPoor(inputFile)
    # getDataPlacePoor(inputFile)
    getDataAgePoor(inputFile)
