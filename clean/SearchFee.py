import time
import CreateData as CD
import pyspark.sql.functions as F
from pyspark.sql import SparkSession
from pyspark.sql.types import DecimalType, IntegerType

"""
è¦ç”¨åˆ°çš„éƒ¨åˆ†è¡¨å¤´ï¼š
    TotalFeeï¼šæ€»è´¹ç”¨
    RealCompï¼šå®é™…è¡¥å¿è´¹ç”¨
    SelfPayï¼šæ‚£è€…è‡ªä»˜è´¹ç”¨
    Countï¼šæŸç§è¯çš„å¼€è¯æ•°ç›®
    FeeSumï¼šè´¹ç”¨é‡‘é¢
    AllowedCompï¼šå¯è¡¥å¿é‡‘é¢
    UnallowedCompï¼šä¸å¯è¡¥å¿é‡‘é¢
    CompRatioï¼šæŠ¥é”€æ¯”ä¾‹
    DTï¼šæ•°æ®å¹´ä»½
"""

# åŠŸèƒ½
'''
# TODO:ç»Ÿè®¡è¿‘ä¸‰å¹´çš„åŒ»ä¿è´¹ç”¨å¼€æ”¯(ä»¥å¹´ä¸ºå•ä½æ˜¾ç¤º)--->RealComp, DT
# TODO:ç»Ÿè®¡è¿‘ä¸‰å¹´è¯å“å¼€æ”¯æƒ…å†µ(ä»¥å¹´ä¸ºå•ä½æ˜¾ç¤º)--->FeeSum, Count, DT 
# TODO:ç»Ÿè®¡è¿‘ä¸‰å¹´åŒ»ä¿è¯å“å¼€æ”¯æƒ…å†µ(å³æ”¯ä»˜é‡‘é¢,æ•°é‡;ä»¥å¹´ä¸ºå•ä½æ˜¾ç¤º)--->AllowedComp, CopRatio, Count, DT
# TODO:ç»Ÿè®¡è¿‘ä¸‰å¹´æ‰¶è´«äººå£æ‚£ç—…çš„åœ°ç†åˆ†å¸ƒ,å¹´é¾„åˆ†å¸ƒ
'''


# TODO: æŒ‰æœˆåˆ†ç±»ç»Ÿè®¡è¯å“ï¼ˆåˆ†ç”²ã€ä¹™ã€ä¸™ï¼‰
# TODOï¼šä¸‰å¹´çš„æ‰¶è´«è´¹ç”¨å¼€æ”¯å¯¹æ¯”
# TODOï¼šå¤šåšå¯¹æ¯”çªå‡ºæ˜¾ç¤ºæ‰¶è´«

# æœºå™¨å­¦ä¹ 
# TODO:é¢„æµ‹æœªæ¥å‡ å¹´åŒ»ä¿æ‰¶è´«è´¹ç”¨çš„æŒç»­å¼€æ”¯æƒ…å†µ
# TODO:é¢„æµ‹æœªæ¥å‡ å¹´æ‰¶è´«äººå£æ‚£ç—…äººç¾¤ä¸»è¦ç”¨è¯æƒ…å†µ(çº³å…¥åŒ»ä¿è¡¥å¿èŒƒå›´)
# TODO:é¢„æµ‹æœªæ¥å‡ å¹´æ‰¶è´«äººå£æ‚£ç—…äººç¾¤çš„åˆ†å¸ƒä½ç½®


# æŸ¥è¯¢ç»Ÿè®¡è¿‘å‡ å¹´çš„åŒ»ä¿å¼€é”€
def getDataMed(path):
    data = spark.read.format('parquet').load(path).select('HosRegisterCode', 'RealComp', 'DT')
    medFee = data.dropDuplicates(subset=['HosRegisterCode']).drop(
        'HosRegisterCode')
    # è®¡ç®—è´¹ç”¨
    allFee = medFee.groupby('DT').agg(F.sum('RealComp').alias('Fee')).orderBy(medFee.DT.desc())
    # é¿å…ç§‘å­¦è®¡æ•°æ³•æ˜¾ç¤º
    allFee = allFee.withColumn('Fee', allFee.Fee.cast(DecimalType(18, 2)))
    allFee.show()


# è·å¾—è¿‘ä¸‰å¹´çš„å»ºæ¡£ç«‹å¡äººå£çš„è¯å“è¡¥åŠ©å¼€æ”¯æƒ…å†µ
def getDataDrugAllowed(path):
    # è·å¾—äººå‘˜ç±»å‹ï¼ˆå»ºæ¡£ç«‹å¡ï¼‰,æ ¹æ®äººå‘˜ç±»å‹ç»Ÿè®¡ç”¨è¯è´¹ç”¨
    data = spark.read.format('parquet').load(path).select('PersonalType', 'AllowedComp', 'DT')
    data = data.where(data.PersonalType == '17') \
        .groupby('DT') \
        .agg(F.sum(data.AllowedComp)
             .alias('Fee')) \
        .orderBy(
        data.DT.desc())
    fee = data.withColumn('Fee', data.Fee.cast(DecimalType(18, 2)))
    fee.show()


# è·å–ğŸ¥‡ä¸‰å¹´æ¥å»ºæ¡£ç«‹å¡äººå£çš„è¯å“ä½¿ç”¨æƒ…å†µ
def getDataDrugPoor(path, tt):
    data = spark.read.format('parquet').load(path)
    # å¤„ç†ç”²ç±»è¯,è·å¾—ç”²ç±»è¯æ¯å¹´çš„å¼€é”€ä»¥åŠæ¯å¹´ä½¿ç”¨çš„æ•°é‡
    data01 = data.select('PersonalType', 'DrugName', 'DT', 'Count', 'FeeSum', 'AllowedComp', 'CompRatio',
                         'CompRatio_Type')
    # & (data01.CompRatio_Type == '{}'.format(tt))

    data01 = data01.where((data01.PersonalType == '17') & (data01.DrugName != '0')) \
        .withColumn("Count", data01.Count.cast(IntegerType())) \
        .withColumn("FeeSum", data01.FeeSum.cast(IntegerType())) \
        .withColumn("DrugName", CD.changeNameUDF(data01.DrugName))
    # data01.show()
    data01 = data01.drop('PersonalType', 'CompRatio_Type', 'CompRatio', 'AllowedComp')

    data01_Fee = data01.drop("Count") \
        .groupby("DrugName") \
        .pivot("DT", ['2017', '2018', '2019']) \
        .agg(F.sum('FeeSum')) \
        .fillna(0)
    data01_Fee = data01_Fee.orderBy(data01_Fee['2019'].desc())
    data_fee = []
    for i in data01_Fee.head(20):
        dd = {'drugName': i['DrugName']}
        tt = [i['2017'], i['2018'], i['2019']]
        dd['drugFee'] = tt
        data_fee.append(dd)

    # data01_Count = data01.drop("FeeSum")\
    #     .groupby("DrugName")\
    #     .pivot("DT", ['2017', '2018', '2019'])\
    #     .agg(F.sum('Count'))\
    #     .fillna(0)
    # data01_Count = data01_Count.orderBy(data01_Count['2019'].desc())
    # data01_Count.show()


# ç»Ÿè®¡è¿‘ä¸‰å¹´æ‚£ç—…æ‰¶è´«äººå£çš„åœ°ç†åˆ†å¸ƒï¼šæ€»å…±21335äººæ¬¡
def getDataPlacePoor(path):
    path01 = 'hdfs://localhost:9000/csv/Join_Canton'
    data01 = spark.read.format('csv').option('header', 'true').load(path01) \
        .where('Level= 04').drop('UpperCode', 'AllName', 'DT') \
        .withColumn('CantonCode', F.split('CantonCode', '\d{4}$')) \
        .withColumn('CantonCode', F.concat_ws("", "CantonCode"))

    data01 = data01.withColumn("CantonName", F.when(data01.CantonName == 'å»ºæ¡£ç«‹å¡äººå‘˜', 'å›ºé˜³å¿å»ºæ¡£ç«‹å¡äººå‘˜') \
                               .otherwise(data01.CantonName))

    data = spark.read.format('parquet').load(path) \
        .select('PersonalType', 'AllName', 'DT', 'HosRegisterCode', 'CantonCode') \
        .dropDuplicates(subset=['HosRegisterCode']) \
        .where('PersonalType = 17') \
        .withColumn('CantonCode', F.split('CantonCode', '\d{4}$')) \
        .withColumn('CantonCode', F.concat_ws("", "CantonCode"))

    data = data.join(data01, on='CantonCode', how='left_outer') \
        .drop('PersonalType', 'Level', 'CantonCode', 'AllName', 'ZoneCode', 'HosRegisterCode') \
        .dropDuplicates(subset=['HosRegisterCode']) \
        .withColumn('Times', F.lit(1)) \
        .groupby('CantonName') \
        .pivot('DT', ['2017', '2018', '2019']) \
        .agg(F.sum('Times')) \
        .fillna(0)
    data = data.orderBy(data['2019'].desc())
    data.show(50)
    data.groupby().sum().show()  # å’Œä¸º21335


# ç»Ÿè®¡è¿‘ä¸‰å¹´æ‚£ç—…æ‰¶è´«äººå£çš„å¹´é¾„åˆ†å¸ƒ
def getDataAgePoor(path, year):
    start = time.time()
    data = spark.read.format('parquet').load(path)
    end = time.time()
    # print(end - start)

    start = time.time()
    data = data.where("DT == {}".format(year)) \
        .select('PersonalType', 'DT', 'HosRegisterCode', 'Age', 'Sex') \
        .dropDuplicates(subset=['HosRegisterCode']) \
        .where('PersonalType = 17') \
        .drop("HosRegisterCode", "PersonalType") \
        .withColumn('Count', F.lit(1))
    end = time.time()
    # print(end - start)

    start = time.time()
    data = data.withColumn('Age', data.Age.cast(IntegerType())).drop("DT")
    data.show()
    data_new = data.groupby("Age") \
        .pivot("Sex", ['ç”·', 'å¥³']) \
        .agg(F.sum('Count')) \
        .fillna(0)
    end = time.time()
    # print(end - start)

    # start = time.time()
    # men_list = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    # women_list = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    # for i in data_new.collect():
    #     p = int(i['Age'] / 10)
    #     if p > 9:
    #         p = 9
    #     men_list[p] += int(i['ç”·'])
    #     women_list[p] += int(i['å¥³'])
    # end = time.time()
    # print(end - start)

    data.createOrReplaceTempView("form")
    start = time.time()
    df = spark.sql("""
    Select Sex,
           sum(case when Age > 90 then 1 else 0 end) as A,
           sum(case when Age between 80 and 89 then 1 else 0 end) as B,
           sum(case when Age between 70 and 79 then 1 else 0 end) as C,
           sum(case when Age between 60 and 69 then 1 else 0 end) as D,
           sum(case when Age between 50 and 59 then 1 else 0 end) as E,
           sum(case when Age between 40 and 49 then 1 else 0 end) as F,
           sum(case when Age between 30 and 39 then 1 else 0 end) as G,
           sum(case when Age between 20 and 29 then 1 else 0 end) as H,
           sum(case when Age between 10 and 19 then 1 else 0 end) as I,
           sum(case when Age between 0 and 9 then 1 else 0 end) as J
    from form
    group by Sex""")
    end = time.time()
    print(end - start)

    start = time.time()
    df_col = df.collect()
    men_list = [df_col[1][1], df_col[1][2], df_col[1][3], df_col[1][4], df_col[1][5], df_col[1][6], df_col[1][7],
                df_col[1][8], df_col[1][9], df_col[1][10], ]
    women_list = [df_col[2][1], df_col[2][2], df_col[2][3], df_col[2][4], df_col[2][5], df_col[2][6], df_col[2][7],
                  df_col[2][8], df_col[2][9], df_col[2][10], ]
    end = time.time()
    print(end - start)
    print(men_list)

    # data_women = data.where(data.Sex == 'å¥³').drop('Sex')
    # data_women = data_women.groupBy('Age') \
    #     .pivot('DT', ['2017', '2018', '2019']) \
    #     .agg(F.sum('Count'))\
    #     .fillna(0)

    # data_men = data.where(data.Sex == 'ç”·').drop('Sex')
    # data_men = data_men.groupBy('Age') \
    #     .pivot('DT', ['2017', '2018', '2019']) \
    #     .agg(F.sum('Count')).fillna('0') \
    #     .fillna(0)
    # data = data.withColumn('Age', data.Age.cast(IntegerType())) \
    #     .orderBy(data['2019'].desc())
    # data.show()
    # data.groupby().sum().show()


# TODO:ç»Ÿè®¡è¿‘ä¸‰å¹´æŸäº›åœ°åŒºçš„æ‰¶è´«äººå£çš„æ‚£ç—…å¹´é¾„åˆ†å¸ƒä»¥åŠç”¨è¯ç±»å‹
def getDataAgeInPlace(path):
    data = spark.read.format('parquet').load(path)


if __name__ == '__main__':
    spark = SparkSession.builder \
        .master("local") \
        .appName("example") \
        .config("spark.debug.maxToStringFields", "100") \
        .config("spark.sql.shuffle.partitions", "400") \
        .config("spark.default.parallelism", "600") \
        .config("spark.sql.auto.repartition", "true") \
        .config("spark.sql.execution.arrow.enabled", "true") \
        .enableHiveSupport() \
        .getOrCreate()

    # æ€»è¡¨è·¯å¾„
    # spark = SparkSession.builder \
    #     .master("local") \
    #     .appName("example") \
    #     .config("spark.debug.maxToStringFields", "1000") \
    #     .getOrCreate()

    inputFile = 'hdfs://localhost:9000/result/form_par'
    # getDataMed(inputFile)
    # getDataDrugAllowed(inputFile)
    # getDataDrugPoor(inputFile, tt="ä¹™ç±»")
    # getDataPlacePoor(inputFile)
    getDataAgePoor(inputFile, '2017')
